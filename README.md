# Implementation of: An Automated Pipeline powered by Generative AI for coherent narrative video generation from stories

The rapid growth of content consumption via digital platforms and streaming has increased the demand for innovative automatic content generation. Narratives, typically consumed through reading, limit the experience to one sense and lack visual and auditory elements. Existing approaches to narrative visualization typically rely on specific themes, fail to generalize across diverse narrative contexts, and often require human intervention. This work presents an automated pipeline for narrative video generation, able to unify visual and auditory aspects for a richer and more coherent representation of stories. Additionally, a reference-free metric is introduced to evaluate the quality of the generated videos. The pipeline comprises four modules that incorporate a feature mechanism and iterative processes. We leverage GPT-4o to segment the story into scenes based on character actions or events, and to extract features from characters and scenes. For each scene, an image is iteratively generated by DALL-E-3, taking into account the extracted features while validating its coherence with the story using CLIP. Results across 6 multilingual narratives of varying genres demonstrate the effectiveness of the proposed solution in automatically generating high-quality narrative videos consisting of synchronized images and audio narrations. The generated videos maintain strong coherence between visual features and audio narrations, achieving an average score of 0.8 out of 1 in the proposed metric regardless of genre or language. Our approach not only advances the field of automatic narrative generation but also opens new avenues for storytelling across diverse cultures and languages, with potential applications in entertainment, education, and marketing.

## Improvements

### Chunk segmentation approach

To address the limitation of having to few scenes in some stories. The new process will partition the story based on a predefined parameter n, so that the number of segments will increase based on such parameter. The new segmentation process follows the steps:

1. Partition the story in n sized blocks. n represents a percentage in length of the story.
2. Each partition will be processed by a new segmentation prompt that will include a request for context in the story at time t.
3. For partitions in time t+1 and onward, the context of segment t-1 will be passed in the prompt.

Currently the process considers an n of 100%, causing some stories to have few segments despite being longer that others. Additionally, shorter fragments of stories have been easier to process for LLMs resulting in the desired format being returned by the models. This new method aims to take advantage of this. The n parameter could also be calculated based on the length of the story.

### Image validation and refinement

Currently the image refinement process only prevents hallucinations from the text-to-image model. The idea is to evaluate the similarity between the prompts describing the characters and scene at that point in the story and the image generated for a that particular scene. Based on this a more detailed evaluation about each individual component could be achieved. Based on each of the similarity score, a refinement prompt could be constructed to enphasize on the lacking of certain features from specific elements in the image. 

A different apporach for refinement would involve the following steps:

0. Have to generate a reference image for each character. In segment 0 handle it with dall-e-3 but for updates in appearance probably use impainting to chage the reference image. Otherwise really expensive.
1. Get similarity scores between each character description and the image.
2. If any score doesn't reach a certain threshold the process is continued.
3. Find and extract each person in the image using some detection model like YOLO.
4. Calculate similarities between each person segmented and each character description. The pair with highest similarity is the one you are refining.
5. Throug impainting, use the reference image to change the appearance of the segmented character.
6. Put the character back in the scene image.

This process is probably not efficient, specially step 4 and the fact that you need a reference image updated across the generation process. It is probably worse that just using the refinement prompt mentioned above but would reduce the cost of the API.

### Metrics

The new validation process should also be taken into account for the metric. Additionally, to compare for character coherence metrics related to clip are used in other works. could also use this to validate the approach. For human evaluation maybe Amazon Mechanical Turk could be an option.

### Scalability

Regarding limitations from papers in this aspect:

1. The Chosen One: Training required and have to tune for each version/change in character.
2. CharacterFactory: Also training required (10 mins tho).
3. StoryMaker: More training.
4. StoryImager, HrPrGan, : Training in Flintstones.
